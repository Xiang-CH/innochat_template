{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "### Import Required Libaried\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "import dotenv \n",
    "#load the environment variables of .env file\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Azure Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credential for using Azure cognitive search\n",
    "search_endpoint = f\"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net/\"\n",
    "search_creds = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                                index_name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "                                credential=search_creds)\n",
    "\n",
    "# Setup the required credential for using Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")     \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text):\n",
    "    return openai.Embedding.create(engine=\"embedding\", input=text)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace(' ', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using ChatGPT Through calling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the query to what you want to ask chatGPT\n",
    "query = \"Where is the capital of India?\"\n",
    "\n",
    "messages = [\n",
    "    {'role' : 'user', 'content' : query }\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=\"chat\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, \n",
    "    n=1)\n",
    "\n",
    "chat_content = chat_completion.choices[0].message.content\n",
    "chat_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the role of ChatGPT \n",
    "- Adding system message\n",
    "- Adding few shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the query to what you want to ask chatGPT\n",
    "query = \"Write a poem about university life\"\n",
    "\n",
    "#change the systemMessage to how you want chatGPT to behave\n",
    "systemMessage = '''You are a Shakespearean writing assistant who speaks in a Shakespearean style. \n",
    "                    You help people come up with creative ideas and content like stories, poems, and songs that use Shakespearean style of writing style, including words like \"thou\" and \"hath‚Äù.\n",
    "                    Here are some example of Shakespeare's style:\n",
    "                    - Romeo, Romeo! Wherefore art thou Romeo?\n",
    "                    - Love looks not with the eyes, but with the mind; and therefore is winged Cupid painted blind.\n",
    "                    - Shall I compare thee to a summer's day? Thou art more lovely and more temperate.'''\n",
    "\n",
    "messages = [\n",
    "    {'role' : 'system', 'content' : systemMessage},\n",
    "    #change the content here to your example question\n",
    "    {'role' : 'user', 'content' : 'Please write a short text turning down an invitation to dinner.'},\n",
    "    #change the content here to your example answer\n",
    "    {'role' : 'assistant', 'content' : '''Dearest,\n",
    "                                        Regretfully, I must decline thy invitation.\n",
    "                                        Prior engagements call me hence. Apologies.'''},\n",
    "    {'role' : 'user', 'content' : query }\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=\"chat\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, \n",
    "    n=1)\n",
    "\n",
    "chat_content = chat_completion.choices[0].message.content\n",
    "print(chat_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing a RAG model\n",
    "\n",
    "### Obtain related information using vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the query to what you want to ask the RAG model\n",
    "query = \"What is SIG\"\n",
    "query_vector = compute_embedding(query)\n",
    "\n",
    "r = search_client.search(query, \n",
    "                        top=3, \n",
    "                        vector=query_vector, \n",
    "                        top_k=50, \n",
    "                        vector_fields=\"embedding\")\n",
    "\n",
    "results = [doc[\"sourcepage\"] + \": \" + nonewlines(doc[\"content\"]) for doc in r]\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the GPT model with query + information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the systemMessage to how you want chatGPT to behave\n",
    "systemMessage = \"\"\"AI Assistant that helps user to answer questions from sources provided. Be brief in your answers.\n",
    "                    Answer ONLY with the facts listed in the list of sources below. \n",
    "                    If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. \n",
    "                    Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. \n",
    "                    Use square brackets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\n",
    "                \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role' : 'system', 'content' : systemMessage},\n",
    "    {'role' : 'user', 'content' : query + \"   Source:\" + \" \".join(results)}\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=\"chat\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, \n",
    "    n=1)\n",
    "\n",
    "chat_content = chat_completion.choices[0].message.content\n",
    "print(chat_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
