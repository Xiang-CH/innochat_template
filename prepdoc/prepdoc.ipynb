{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PDF Documents for RAG \n",
    "\n",
    "*Code adapded from: https://github.com/Azure-Samples/azure-search-openai-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import re\n",
    "\n",
    "import openai\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "import dotenv \n",
    "#load the environment variables of .env file\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Search Index\n",
    "\n",
    "- Search Index is the means by which data is organized and structured so that search engines can generate relevant search results. Search indexing can transform any and all data and file types into searchable data. \n",
    "\n",
    "    - In an abstract sense we are creating a dictionary so that a key can correspond to some values. In our case the embedding is the key and the other field are the values \n",
    "\n",
    "- If we are using Text search instead of Vector search, search index will be setup in a structure called **Inverted Index**. Read more at: https://www.geeksforgeeks.org/inverted-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credentials for using Azure cognitive search\n",
    "search_endpoint = f\"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net/\"\n",
    "search_creds = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "index_client = SearchIndexClient(endpoint= search_endpoint, credential=search_creds)\n",
    "\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "\n",
    "if index_name not in index_client.list_index_names():\n",
    "    # Define a search index\n",
    "    index = SearchIndex(\n",
    "                name=index_name,\n",
    "                fields=[\n",
    "                    SimpleField(name=\"id\", type=\"Edm.String\", key=True),                                              #Unique id field\n",
    "                    SearchableField(name=\"content\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"),                 #Content field\n",
    "                    SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "                                hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,\n",
    "                                vector_search_dimensions=1536, vector_search_configuration=\"default\"),                #Searchable field: Embedding\n",
    "                    SimpleField(name=\"sourcepage\", type=\"Edm.String\", filterable=True, facetable=True),               #Blob name field\n",
    "                    SimpleField(name=\"sourcefile\", type=\"Edm.String\", filterable=True, facetable=True)                #File name field\n",
    "                ],\n",
    "                \n",
    "                #Additional Semantic Setting if we want to use the semantic search feature in the future\n",
    "                semantic_settings=SemanticSettings(\n",
    "                    configurations=[SemanticConfiguration(\n",
    "                        name='default',\n",
    "                        prioritized_fields=PrioritizedFields(title_field=None, prioritized_content_fields=[SemanticField(field_name='content')]))]),\n",
    "                    vector_search=VectorSearch(\n",
    "                        algorithm_configurations=[\n",
    "                            VectorSearchAlgorithmConfiguration(\n",
    "                                name=\"default\",\n",
    "                                kind=\"hnsw\",\n",
    "                                hnsw_parameters=HnswParameters(metric=\"cosine\") \n",
    "                            )\n",
    "                        ]\n",
    "                    )        \n",
    "                )   \n",
    "    # Create the search index\n",
    "    index_client.create_index(index)\n",
    "else:\n",
    "    print(f\"Index: '{index_name}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embedding model\n",
    "\n",
    "- Embedding is the process of turning text in to a very high dimensional vector, where each dimensional represents a feature of the text. Therefore the semantics of the text can be emmbedded in the vector representation\n",
    "\n",
    "- There we simplifies the OpenAI Embedding API call into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credential for using Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")      \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "def compute_embedding(text):\n",
    "    return openai.Embedding.create(engine=\"embedding\", input=text)[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from documents\n",
    "\n",
    "- Using a python library **pdfreader** to extract the text of our pdf documents and stores it in a special data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./data/\" + \"The_Innovation_Wings.pdf\" #Change to name of yout file (make sure the file name does not include any space)\n",
    "\n",
    "offset = 0       #The character count from the start of the document\n",
    "page_map = []    #List of turples: (page_num, offset, page_text)\n",
    "\n",
    "print(f\"Extracting text from '{filename}' using PdfReader\")\n",
    "\n",
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for page_num, p in enumerate(pages):\n",
    "    page_text = p.extract_text()\n",
    "    page_map.append((page_num, offset, page_text))\n",
    "    offset += len(page_text)\n",
    "    \n",
    "page_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section the extracted text\n",
    "\n",
    "- Sectioning means segmenting/splitting text before indexing them, it brings a few benefits:\n",
    "\n",
    "    - Improved Efficiency: Faster and more efficient vector search. \n",
    "\n",
    "    - Enhanced Precision: By indexing smaller text segments the search engine can capture the fine-grained semantic information present in the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LENGTH = 1000\n",
    "SENTENCE_SEARCH_LIMIT = 100\n",
    "SECTION_OVERLAP = 100\n",
    "\n",
    "\n",
    "def filename_to_id(filename): \n",
    "    filename_ascii = re.sub(\"[^0-9a-zA-Z_-]\", \"_\", filename)\n",
    "    filename_hash = base64.b16encode(filename.encode('utf-8')).decode('ascii')\n",
    "    return f\"file-{filename_ascii}-{filename_hash}\"\n",
    "\n",
    "def split_text(page_map):\n",
    "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \" \"]\n",
    "\n",
    "    def find_page(offset):\n",
    "        l = len(page_map)\n",
    "        for i in range(l - 1):\n",
    "            if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
    "                return i\n",
    "        return l - 1\n",
    "\n",
    "    all_text = \"\".join(p[2] for p in page_map)\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "    while start + SECTION_OVERLAP < length:\n",
    "        last_word = -1\n",
    "        end = start + MAX_SECTION_LENGTH\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
    "                if all_text[end] in WORDS_BREAKS:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "                end = last_word # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        last_word = -1\n",
    "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
    "            if all_text[start] in WORDS_BREAKS:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        yield (section_text, find_page(start))\n",
    "\n",
    "        last_table_start = section_text.rfind(\"<table\")\n",
    "        if (last_table_start > 2 * SENTENCE_SEARCH_LIMIT and last_table_start > section_text.rfind(\"</table\")):\n",
    "            # If the section ends with an unclosed table, we need to start the next section with the table.\n",
    "            # If table starts inside SENTENCE_SEARCH_LIMIT, we ignore it, as that will cause an infinite loop for tables longer than MAX_SECTION_LENGTH\n",
    "            # If last table starts inside SECTION_OVERLAP, keep overlapping\n",
    "            start = min(end - SECTION_OVERLAP, start + last_table_start)\n",
    "        else:\n",
    "            start = end - SECTION_OVERLAP\n",
    "        \n",
    "    if start + SECTION_OVERLAP < end:\n",
    "        yield (all_text[start:end], find_page(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "file_id = filename_to_id(filename)\n",
    "for i, (content, pagenum) in enumerate(split_text(page_map)):\n",
    "    section = {\n",
    "        \"id\": f\"{file_id}-page-{i}\",\n",
    "        \"content\": content,\n",
    "        \"embedding\": compute_embedding(content),\n",
    "        \"sourcepage\": os.path.splitext(os.path.basename(filename))[0] + f\"-{pagenum}\" + \".pdf\",\n",
    "        \"sourcefile\": filename\n",
    "    }\n",
    "    sections.append(section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index sections\n",
    "\n",
    "- Uploading the sections to the search index we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                                    index_name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "                                    credential=search_creds)\n",
    "i = 0\n",
    "batch = []\n",
    "#index 1000 sections at a time\n",
    "for s in sections:\n",
    "    batch.append(s)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "        batch = []\n",
    "        \n",
    "#index the remaining sections\n",
    "if len(batch) > 0:\n",
    "    results = search_client.upload_documents(documents=batch)\n",
    "    succeeded = sum([1 for r in results if r.succeeded])\n",
    "    print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split PDF into single page Blobs\n",
    "\n",
    "- Creating a PDF file for each page of the document so that we can cited individual pages in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for i in range(len(pages)):\n",
    "    blob_name = os.path.splitext(os.path.basename(filename))[0] + f\"-{i}\" + \".pdf\"\n",
    "    print(f\"\\tCreating blob for page {i} -> {blob_name}\")\n",
    "    writer = PdfWriter()\n",
    "    writer.add_page(pages[i])\n",
    "    writer.write(\"../database/\"+blob_name)\n",
    "    writer.write(\"../app/backend/static/database/\"+blob_name)\n",
    "    writer.write(\"../app/frontend/public/database/\"+blob_name)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Index\n",
    "\n",
    "- Searching the vector index using kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" \" #your query keywords\n",
    "query_vector = compute_embedding(query)\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace(' ', ' ').replace('\\r', ' ')\n",
    "\n",
    "r = search_client.search(query, \n",
    "                        top=3, \n",
    "                        vector=query_vector, \n",
    "                        top_k=50, \n",
    "                        vector_fields=\"embedding\")\n",
    "\n",
    "results = [doc[\"sourcepage\"] + \": \" + nonewlines(doc[\"content\"]) for doc in r]\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
