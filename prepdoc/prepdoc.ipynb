{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf==3.9.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: azure-identity==1.13.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.13.0)\n",
      "Requirement already satisfied: azure-search-documents==11.4.0b6 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (11.4.0b6)\n",
      "Requirement already satisfied: openai[datalib]==0.27.8 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.27.8)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from pypdf==3.9.0->-r requirements.txt (line 1)) (4.6.3)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.11.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-identity==1.13.0->-r requirements.txt (line 2)) (1.27.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-identity==1.13.0->-r requirements.txt (line 2)) (39.0.1)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.20.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-identity==1.13.0->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-identity==1.13.0->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-identity==1.13.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-search-documents==11.4.0b6->-r requirements.txt (line 3)) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from azure-search-documents==11.4.0b6->-r requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2.29.0)\n",
      "Requirement already satisfied: tqdm in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (3.8.4)\n",
      "Requirement already satisfied: numpy in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.26.0)\n",
      "Requirement already satisfied: pandas>=1.2.3 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.3.4)\n",
      "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2.0.3.230814)\n",
      "Requirement already satisfied: openpyxl>=3.0.7 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openai[datalib]==0.27.8->-r requirements.txt (line 4)) (3.0.10)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from cryptography>=2.5->azure-identity==1.13.0->-r requirements.txt (line 2)) (1.15.1)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from msal<2.0.0,>=1.20.0->azure-identity==1.13.0->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity==1.13.0->-r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: et-xmlfile in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from openpyxl>=3.0.7->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.3->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.3->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2022.7)\n",
      "Requirement already satisfied: types-pytz>=2022.1.1 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2023.3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai[datalib]==0.27.8->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: pycparser in /Users/chenxiang/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity==1.13.0->-r requirements.txt (line 2)) (2.21)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import re\n",
    "\n",
    "import openai\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "import dotenv \n",
    "#load the environment variables of .env file\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credentials for using Azure cognitive search\n",
    "search_endpoint = f\"https://{os.getenv('AZURE_SEARCH_ENDPOINT')}.search.windows.net/\"\n",
    "search_creds = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "index_client = SearchIndexClient(endpoint= search_endpoint, credential=search_creds)\n",
    "\n",
    "# Define a search index\n",
    "index = SearchIndex(\n",
    "            name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "            fields=[\n",
    "                SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "                SearchableField(name=\"content\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"),\n",
    "                SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                            hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,\n",
    "                            vector_search_dimensions=1536, vector_search_configuration=\"default\"),\n",
    "                SimpleField(name=\"sourcepage\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "                SimpleField(name=\"sourcefile\", type=\"Edm.String\", filterable=True, facetable=True)\n",
    "            ],\n",
    "            semantic_settings=SemanticSettings(\n",
    "                configurations=[SemanticConfiguration(\n",
    "                    name='default',\n",
    "                    prioritized_fields=PrioritizedFields(title_field=None, prioritized_content_fields=[SemanticField(field_name='content')]))]),\n",
    "                vector_search=VectorSearch(\n",
    "                    algorithm_configurations=[\n",
    "                        VectorSearchAlgorithmConfiguration(\n",
    "                            name=\"default\",\n",
    "                            kind=\"hnsw\",\n",
    "                            hnsw_parameters=HnswParameters(metric=\"cosine\") \n",
    "                        )\n",
    "                    ]\n",
    "                )        \n",
    "            )\n",
    "\n",
    "# Create the search index\n",
    "index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credential for using Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")        \n",
    "openai_Service = os.getenv(\"AZURE_OPENAI_SERVICE\")\n",
    "openai.api_base = f\"https://{openai_Service}.openai.azure.com\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "def compute_embedding(text):\n",
    "    return openai.Embedding.create(engine=\"embedding\", input=text)[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./data/\" + \" \" #your file name\n",
    "\n",
    "offset = 0\n",
    "page_map = []\n",
    "\n",
    "print(f\"Extracting text from '{filename}' using PdfReader\")\n",
    "\n",
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for page_num, p in enumerate(pages):\n",
    "    page_text = p.extract_text()\n",
    "    page_map.append((page_num, offset, page_text))\n",
    "    offset += len(page_text)\n",
    "    \n",
    "page_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LENGTH = 1000\n",
    "SENTENCE_SEARCH_LIMIT = 100\n",
    "SECTION_OVERLAP = 100\n",
    "\n",
    "\n",
    "def filename_to_id(filename): \n",
    "    filename_ascii = re.sub(\"[^0-9a-zA-Z_-]\", \"_\", filename)\n",
    "    filename_hash = base64.b16encode(filename.encode('utf-8')).decode('ascii')\n",
    "    return f\"file-{filename_ascii}-{filename_hash}\"\n",
    "\n",
    "def split_text(page_map):\n",
    "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]\n",
    "\n",
    "    def find_page(offset):\n",
    "        l = len(page_map)\n",
    "        for i in range(l - 1):\n",
    "            if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
    "                return i\n",
    "        return l - 1\n",
    "\n",
    "    all_text = \"\".join(p[2] for p in page_map)\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "    while start + SECTION_OVERLAP < length:\n",
    "        last_word = -1\n",
    "        end = start + MAX_SECTION_LENGTH\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
    "                if all_text[end] in WORDS_BREAKS:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "                end = last_word # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        last_word = -1\n",
    "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
    "            if all_text[start] in WORDS_BREAKS:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        yield (section_text, find_page(start))\n",
    "\n",
    "        last_table_start = section_text.rfind(\"<table\")\n",
    "        if (last_table_start > 2 * SENTENCE_SEARCH_LIMIT and last_table_start > section_text.rfind(\"</table\")):\n",
    "            # If the section ends with an unclosed table, we need to start the next section with the table.\n",
    "            # If table starts inside SENTENCE_SEARCH_LIMIT, we ignore it, as that will cause an infinite loop for tables longer than MAX_SECTION_LENGTH\n",
    "            # If last table starts inside SECTION_OVERLAP, keep overlapping\n",
    "            start = min(end - SECTION_OVERLAP, start + last_table_start)\n",
    "        else:\n",
    "            start = end - SECTION_OVERLAP\n",
    "        \n",
    "    if start + SECTION_OVERLAP < end:\n",
    "        yield (all_text[start:end], find_page(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "file_id = filename_to_id(filename)\n",
    "for i, (content, pagenum) in enumerate(split_text(page_map)):\n",
    "    section = {\n",
    "        \"id\": f\"{file_id}-page-{i}\",\n",
    "        \"content\": content,\n",
    "        \"embedding\": compute_embedding(content),\n",
    "        \"sourcepage\": os.path.splitext(os.path.basename(filename))[0] + f\"-{pagenum}\" + \".pdf\",\n",
    "        \"sourcefile\": filename\n",
    "    }\n",
    "    sections.append(section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                                    index_name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "                                    credential=search_creds)\n",
    "i = 0\n",
    "batch = []\n",
    "#index 1000 sections at a time\n",
    "for s in sections:\n",
    "    batch.append(s)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "        batch = []\n",
    "        \n",
    "#index the remaining sections\n",
    "if len(batch) > 0:\n",
    "    results = search_client.upload_documents(documents=batch)\n",
    "    succeeded = sum([1 for r in results if r.succeeded])\n",
    "    print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split PDF into blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for i in range(len(pages)):\n",
    "    blob_name = os.path.splitext(os.path.basename(filename))[0] + f\"-{i}\" + \".pdf\"\n",
    "    print(f\"\\tCreating blob for page {i} -> {blob_name}\")\n",
    "    writer = PdfWriter()\n",
    "    writer.add_page(pages[i])\n",
    "    writer.write(\"../DataBase/\"+blob_name)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" \" #your query keywords\n",
    "query_vector = compute_embedding(query)\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "r = search_client.search(query, \n",
    "                        top=3, \n",
    "                        vector=query_vector, \n",
    "                        top_k=50, \n",
    "                        vector_fields=\"embedding\")\n",
    "\n",
    "results = [doc[\"sourcepage\"] + \": \" + nonewlines(doc[\"content\"]) for doc in r]\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
