{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PDF Documents for RAG \n",
    "\n",
    "*Code adapded from: https://github.com/Azure-Samples/azure-search-openai-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import re\n",
    "\n",
    "import openai\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "import dotenv \n",
    "#load the environment variables of .env file\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credentials for using Azure cognitive search\n",
    "search_endpoint = f\"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net/\"\n",
    "search_creds = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "index_client = SearchIndexClient(endpoint= search_endpoint, credential=search_creds)\n",
    "\n",
    "# Define a search index\n",
    "index = SearchIndex(\n",
    "            name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "            fields=[\n",
    "                SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "                SearchableField(name=\"content\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"),\n",
    "                SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                            hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,\n",
    "                            vector_search_dimensions=1536, vector_search_configuration=\"default\"),\n",
    "                SimpleField(name=\"sourcepage\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "                SimpleField(name=\"sourcefile\", type=\"Edm.String\", filterable=True, facetable=True)\n",
    "            ],\n",
    "            semantic_settings=SemanticSettings(\n",
    "                configurations=[SemanticConfiguration(\n",
    "                    name='default',\n",
    "                    prioritized_fields=PrioritizedFields(title_field=None, prioritized_content_fields=[SemanticField(field_name='content')]))]),\n",
    "                vector_search=VectorSearch(\n",
    "                    algorithm_configurations=[\n",
    "                        VectorSearchAlgorithmConfiguration(\n",
    "                            name=\"default\",\n",
    "                            kind=\"hnsw\",\n",
    "                            hnsw_parameters=HnswParameters(metric=\"cosine\") \n",
    "                        )\n",
    "                    ]\n",
    "                )        \n",
    "            )\n",
    "\n",
    "# Create the search index\n",
    "index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credential for using Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")        \n",
    "openai_Service = os.getenv(\"AZURE_OPENAI_SERVICE\")\n",
    "openai.api_base = f\"https://{openai_Service}.openai.azure.com\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "def compute_embedding(text):\n",
    "    return openai.Embedding.create(engine=\"embedding\", input=text)[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./data/\" + \" \" #your file name (make sure the file name does not include any space)\n",
    "\n",
    "offset = 0\n",
    "page_map = []\n",
    "\n",
    "print(f\"Extracting text from '{filename}' using PdfReader\")\n",
    "\n",
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for page_num, p in enumerate(pages):\n",
    "    page_text = p.extract_text()\n",
    "    page_map.append((page_num, offset, page_text))\n",
    "    offset += len(page_text)\n",
    "    \n",
    "page_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LENGTH = 1000\n",
    "SENTENCE_SEARCH_LIMIT = 100\n",
    "SECTION_OVERLAP = 100\n",
    "\n",
    "\n",
    "def filename_to_id(filename): \n",
    "    filename_ascii = re.sub(\"[^0-9a-zA-Z_-]\", \"_\", filename)\n",
    "    filename_hash = base64.b16encode(filename.encode('utf-8')).decode('ascii')\n",
    "    return f\"file-{filename_ascii}-{filename_hash}\"\n",
    "\n",
    "def split_text(page_map):\n",
    "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \" \"]\n",
    "\n",
    "    def find_page(offset):\n",
    "        l = len(page_map)\n",
    "        for i in range(l - 1):\n",
    "            if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
    "                return i\n",
    "        return l - 1\n",
    "\n",
    "    all_text = \"\".join(p[2] for p in page_map)\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "    while start + SECTION_OVERLAP < length:\n",
    "        last_word = -1\n",
    "        end = start + MAX_SECTION_LENGTH\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
    "                if all_text[end] in WORDS_BREAKS:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "                end = last_word # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        last_word = -1\n",
    "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
    "            if all_text[start] in WORDS_BREAKS:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        yield (section_text, find_page(start))\n",
    "\n",
    "        last_table_start = section_text.rfind(\"<table\")\n",
    "        if (last_table_start > 2 * SENTENCE_SEARCH_LIMIT and last_table_start > section_text.rfind(\"</table\")):\n",
    "            # If the section ends with an unclosed table, we need to start the next section with the table.\n",
    "            # If table starts inside SENTENCE_SEARCH_LIMIT, we ignore it, as that will cause an infinite loop for tables longer than MAX_SECTION_LENGTH\n",
    "            # If last table starts inside SECTION_OVERLAP, keep overlapping\n",
    "            start = min(end - SECTION_OVERLAP, start + last_table_start)\n",
    "        else:\n",
    "            start = end - SECTION_OVERLAP\n",
    "        \n",
    "    if start + SECTION_OVERLAP < end:\n",
    "        yield (all_text[start:end], find_page(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "file_id = filename_to_id(filename)\n",
    "for i, (content, pagenum) in enumerate(split_text(page_map)):\n",
    "    section = {\n",
    "        \"id\": f\"{file_id}-page-{i}\",\n",
    "        \"content\": content,\n",
    "        \"embedding\": compute_embedding(content),\n",
    "        \"sourcepage\": os.path.splitext(os.path.basename(filename))[0] + f\"-{pagenum}\" + \".pdf\",\n",
    "        \"sourcefile\": filename\n",
    "    }\n",
    "    sections.append(section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                                    index_name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "                                    credential=search_creds)\n",
    "i = 0\n",
    "batch = []\n",
    "#index 1000 sections at a time\n",
    "for s in sections:\n",
    "    batch.append(s)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "        batch = []\n",
    "        \n",
    "#index the remaining sections\n",
    "if len(batch) > 0:\n",
    "    results = search_client.upload_documents(documents=batch)\n",
    "    succeeded = sum([1 for r in results if r.succeeded])\n",
    "    print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split PDF into blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for i in range(len(pages)):\n",
    "    blob_name = os.path.splitext(os.path.basename(filename))[0] + f\"-{i}\" + \".pdf\"\n",
    "    print(f\"\\tCreating blob for page {i} -> {blob_name}\")\n",
    "    writer = PdfWriter()\n",
    "    writer.add_page(pages[i])\n",
    "    writer.write(\"../database/\"+blob_name)\n",
    "    writer.write(\"../app/backend/static/database/\"+blob_name)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" \" #your query keywords\n",
    "query_vector = compute_embedding(query)\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace(' ', ' ').replace('\\r', ' ')\n",
    "\n",
    "r = search_client.search(query, \n",
    "                        top=3, \n",
    "                        vector=query_vector, \n",
    "                        top_k=50, \n",
    "                        vector_fields=\"embedding\")\n",
    "\n",
    "results = [doc[\"sourcepage\"] + \": \" + nonewlines(doc[\"content\"]) for doc in r]\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
